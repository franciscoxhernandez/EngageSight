# EngageSight â€” AI Engagement Recognition Tool | Version 1.0

## ğŸ” Overview

**EngageSight** is an AI-powered tool that automatically analyzes participantsâ€™ engagement levels during online meetings.  
It combines **AWS Rekognition**, **Lambda**, **DynamoDB**, and **S3** in an event-driven architecture, with a web interface 
hosted on **AWS EC2** for intuitive video uploads and results visualization.

Originally developed, and currently being further refined, as part of the ongoing PhD research of **Francisco HernÃ¡ndez** 
at the [**Chair for Ergonomics and Innovation**](https://www.tu-chemnitz.de/mb/ArbeitsWiss/index.php), **Technische UniversitÃ¤t Chemnitz**.

**EngageSight** explores how facial expressions and visual attention can be used to understand engagement during virtual meetings, **without 
tracking, or storing any biometric data**.

---

## ğŸ“¦ About This Repository

This repository contains the **frontend application** for EngageSight â€” including the video upload, frame extraction, and results visualization logic (Flask + HTML/CSS/JS).

The backend components, such as **AWS Lambda** functions, **Rekognition** integration, and **Terraform** infrastructure, are part of a **proprietary research setup** developed for the PhD project.

A short **demo video** is available to illustrate how the system works in action 
- ğŸ”— [**Link to video**](https://engagesight-demo-video.s3.us-east-1.amazonaws.com/EngageSight_Video_GitHub.mp4 )

If you are interested in a **collaboration**, **demo**, or **research access**, please contact me directly:
 
- ğŸ”— [**LinkedIn**](https://www.linkedin.com/in/francisco-hernandez-col-ger/) 

### ğŸ”„ How it works
1. **Video Upload**
- The user opens the EngageSight website (hosted on AWS EC2).
- A meeting video (.mp4, .mov, .avi) is uploaded using the web form.
- The user selects how often to extract frames â€” every 5, 10, 20, 30, or 60 seconds.
2.	**Frame Extraction**
- The Flask app calls the `video_to_images.py script
- OpenCV processes the video locally on the EC2 instance.
- Frames are saved temporarily and uploaded to the S3 bucket `engagesight-images`.
3. **AI Analysis (Automated via AWS)**
- Each uploaded frame automatically triggers the `EngageSightAnalysis` Lambda function.
- The Lambda function uses **AWS Rekognition** to detect:
  - Engagement indicators such as; 
    - Facial expressions and emotion intensity 
    - Attention (looking at camera)
    - Speaking activity and mood indicators
- Results are stored as .json files in S3 â†’ `engagesight-results` and 
summary data in DynamoDB â†’ `EngageSightParticipants`.
4. **Annotated Visualization**
- The `engagesight-annotator-trigger` Lambda function triggers the EC2 `engagesight-drawboxes` 
to draw bounding boxes and labels.
  - The draw boxes are classified in: 
    - ğŸŸ¢ **GREEN**  â†’ High Engagement 
    - ğŸŸ¡ **YELLOW** â†’ Medium Engagement 
    - ğŸ”´ **RED** â†’ Low Engagement 
    - `ADD HERE AND EXAMPLE OF GREEN YELLOW AND RED`
- Annotated images are uploaded to S3 â†’ `engagesight-annotated`.
- The web app displays these annotated images directly in the browser.
5. **Results Download**
- Users can download:
  - ğŸ“¸ Annotated Images from `engagesight-annotated`
  - ğŸ“Š Engagement CSV generated from `EngageSightParticipants` (DynamoDB)
    - This shows the Name of each participant
    - On how many frames was present 
    - Last engagement level 
    - Total Engagement
    - Average Engagement 
  - All downloads use pre-generated S3 URLs returned by Flask. 
- After download all files in the buckets and the dynamodb are deleted 

### âš ï¸ Usage Recommendations
- Limit videos to **10 participants or fewer** for best performance.  
- Each participant must have a **unique name** â€” duplicate names will be merged.  
- **Only one screen** should be visible in the recording.  
- **No audio or facial tracking** is analyzed â€” only frame-by-frame visual data are processed locally. 

### ğŸ“ Summary
EngageSight offers a **fully automated AI workflow** from video upload to engagement visualization.

### ğŸ§© Architecture Overview
```

```

## ğŸ§® Simplified Frontend Logic

| **Feature** | **Description** |
|--------------|-----------------|
| ğŸ§© **split_video_to_images()** | Handles all uploads and video-to-frame splitting. Each frame is uploaded to S3 automatically. |
| ğŸª£ **Real URLs returned** | Flask returns a JSON list of direct S3 URLs (public or presigned) back to the frontend. |
| ğŸ§¼ **Auto-clean** | After processing, temporary video files are automatically deleted from EC2 to save space. |
| ğŸ’¬ **Clear logs** | Console logs show video name, duration, chosen interval, and progress. |
| ğŸš¨ **Error handling** | The app returns structured error messages (missing file, invalid format, AWS issues, etc.). |
| ğŸ–¼ï¸ **Download Annotated Images** | Lists all `.png` / `.jpg` files from `s3://engagesight-annotated/annotated/`. |
| ğŸ“Š **Download Participant CSV** | Exports all DynamoDB table entries into a downloadable CSV. |

----

## ğŸ“œ License

This repository is shared for **academic and research visibility** only.  
All rights to the EngageSight architecture, backend pipeline, and associated AWS infrastructure remain with **Francisco HernÃ¡ndez** and the **Chair of Ergonomics and Innovation Management, Technische UniversitÃ¤t Chemnitz**.

You may **view, clone, and reference** this code for non-commercial purposes.  
Reproduction, distribution, or commercial use of the full EngageSight system (including backend and AI logic) requires **explicit written permission** from the author.

Â© 2025 Francisco HernÃ¡ndez. All rights reserved.